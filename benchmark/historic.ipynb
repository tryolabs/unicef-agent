{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore[all]\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langfuse import Langfuse\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    secret_key=os.environ[\"LANGFUSE_SECRET_KEY\"],\n",
    "    public_key=os.environ[\"LANGFUSE_PUBLIC_KEY\"],\n",
    "    host=os.environ[\"LANGFUSE_HOST\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get scores from the last month\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "\n",
    "scores = []\n",
    "page = 1\n",
    "limit = 100\n",
    "\n",
    "while True:\n",
    "    scores_batch = langfuse.api.score_v_2.get(\n",
    "        limit=limit,\n",
    "        page=page,\n",
    "        from_timestamp=start_date,\n",
    "        to_timestamp=end_date\n",
    "    )\n",
    "\n",
    "    if not scores_batch.data:\n",
    "        break\n",
    "\n",
    "    scores.extend(scores_batch.data)\n",
    "    page += 1\n",
    "\n",
    "    # Break if we got fewer results than the limit (last page)\n",
    "    if len(scores_batch.data) < limit:\n",
    "        break\n",
    "\n",
    "print(f\"{len(scores)} Scores Loaded from the last month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score:\n",
    "    def __init__(self, name: str, value: float, updated_at: datetime, string_value: str) -> None:\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        self.updated_at = updated_at\n",
    "        self.string_value = string_value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return a string representation of the score.\"\"\"\n",
    "        if self.string_value:\n",
    "            return f\"{self.name}: {self.value} ({self.string_value})\"\n",
    "        else:\n",
    "            return f\"{self.name}: {self.value}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Return a string representation of the score.\"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "# Convert raw score data to Score objects\n",
    "converted_scores: list[Score] = []\n",
    "for score in scores.data:\n",
    "    converted_score = Score(\n",
    "        name=score.name,\n",
    "        value=score.value if score.value is not None else 0,\n",
    "        updated_at=score.updated_at,\n",
    "        string_value=getattr(score, \"string_value\", \"\"),\n",
    "    )\n",
    "    converted_scores.append(converted_score)\n",
    "\n",
    "scores_by_name: defaultdict[str, list[Score]] = defaultdict(list)\n",
    "\n",
    "for score in converted_scores:\n",
    "    scores_by_name[score.name].append(score)\n",
    "\n",
    "# Sort each group by updated_at\n",
    "for name in scores_by_name:\n",
    "    scores_by_name[name].sort(key=lambda x: x.updated_at)\n",
    "\n",
    "metrics = [\"answer_correctness\", \"conciseness\", \"faithfulness\", \"completeness\"]\n",
    "\n",
    "answer_correctness_scores = scores_by_name[\"answer_correctness\"]  # for numeric responses\n",
    "conciseness_scores = scores_by_name[\"conciseness\"]  # for text responses\n",
    "faithfulness_scores = scores_by_name[\"faithfulness\"]  # for text responses\n",
    "completeness_scores = scores_by_name[\"completeness\"]  # for text responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_to_dataframe(scores_dict: dict[str, list[Score]]) -> pd.DataFrame:\n",
    "    data: list[dict[str, Any]] = []\n",
    "    for metric_name, scores_list in scores_dict.items():\n",
    "        for score in scores_list:\n",
    "            # Convert updated_at back to datetime if it's a string\n",
    "            if isinstance(score.updated_at, str):\n",
    "                updated_at = datetime.fromisoformat(score.updated_at.replace(\"Z\", \"+00:00\"))\n",
    "            else:\n",
    "                updated_at = score.updated_at\n",
    "\n",
    "            benchmark_run_time = updated_at.replace(minute=0, second=0, microsecond=0)\n",
    "\n",
    "            data.append(\n",
    "                {\n",
    "                    \"metric\": metric_name,\n",
    "                    \"value\": score.value,\n",
    "                    \"string_value\": score.string_value if score.string_value != \"\" else None,\n",
    "                    \"timestamp\": updated_at,\n",
    "                    \"benchmark_run\": benchmark_run_time,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "scores_df = scores_to_dataframe(scores_by_name)\n",
    "\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total scores: {len(scores_df)}\")\n",
    "print(f\"Date range: {scores_df['timestamp'].min()} to {scores_df['timestamp'].max()}\")  # type: ignore[index]\n",
    "print(f\"Metrics: {scores_df['metric'].unique()}\")\n",
    "print(f\"Unique benchmark runs: {scores_df['benchmark_run'].nunique()}\")\n",
    "print(\"\\nScore counts by metric:\")\n",
    "print(scores_df[\"metric\"].value_counts())\n",
    "print(\"\\nBenchmark runs:\")\n",
    "print(scores_df['benchmark_run'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_metrics(scores_df: pd.DataFrame, metrics: list[str]) -> None:\n",
    "    \"\"\"Plot categorical metrics using stacked bar charts.\"\"\"\n",
    "    categorical_metrics: list[str] = []\n",
    "    for metric in metrics:\n",
    "        metric_data = scores_df[scores_df[\"metric\"] == metric]\n",
    "        if len(metric_data) > 0:\n",
    "            categorical_metrics.append(metric)\n",
    "\n",
    "    if not categorical_metrics:\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for metric in categorical_metrics:\n",
    "        metric_data = scores_df[scores_df[\"metric\"] == metric].sort_values(\"benchmark_run\")\n",
    "        # Group by benchmark run and count occurrences\n",
    "        category_counts = metric_data.groupby([\"benchmark_run\", \"string_value\"]).size().unstack(fill_value=0)\n",
    "\n",
    "        # Create stacked bar chart\n",
    "        category_counts.plot(kind=\"bar\", stacked=True, width=0.8, alpha=0.8, ax=ax)\n",
    "        ax.set_title(f\"{metric.replace('_', ' ').title()}\")\n",
    "        ax.set_xlabel(\"Benchmark Run\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.legend(title=\"Category\")\n",
    "        ax.tick_params(axis=\"x\", rotation=0)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_numeric_metrics(scores_df: pd.DataFrame, metrics: list[str]) -> None:\n",
    "    \"\"\"Plot numeric metrics using line plots with average and shaded standard deviation.\"\"\"\n",
    "    numeric_metrics = []\n",
    "    for metric in metrics:\n",
    "        metric_data = scores_df[scores_df[\"metric\"] == metric]\n",
    "        if len(metric_data) > 0:\n",
    "            numeric_metrics.append(metric)\n",
    "\n",
    "    if not numeric_metrics:\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(\"Numeric Metrics Evolution Over Time\", fontsize=16)\n",
    "\n",
    "    for i, metric in enumerate(numeric_metrics):\n",
    "        metric_data = scores_df[scores_df[\"metric\"] == metric].sort_values(\"benchmark_run\")\n",
    "\n",
    "        # Calculate average and std for each benchmark run\n",
    "        run_stats = metric_data.groupby(\"benchmark_run\")[\"value\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "        run_stats[\"std\"] = run_stats[\"std\"].fillna(0)  # Handle single-value runs\n",
    "\n",
    "        # Plot average line\n",
    "        axes[i].plot(\n",
    "            run_stats[\"benchmark_run\"],\n",
    "            run_stats[\"mean\"],\n",
    "            marker=\"o\",\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            color='blue'\n",
    "        )\n",
    "\n",
    "        # Fill area for standard deviation\n",
    "        axes[i].fill_between(\n",
    "            run_stats[\"benchmark_run\"],\n",
    "            run_stats[\"mean\"] - run_stats[\"std\"],\n",
    "            run_stats[\"mean\"] + run_stats[\"std\"],\n",
    "            alpha=0.3,\n",
    "            color='blue',\n",
    "            label='Â± 1 std'\n",
    "        )\n",
    "\n",
    "        axes[i].set_title(f\"{metric.replace('_', ' ').title()}\")\n",
    "        axes[i].set_xlabel(\"Benchmark Run\")\n",
    "        axes[i].set_ylabel(\"Score\")\n",
    "        axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Hide unused subplots if there are fewer than 3 metrics\n",
    "    for i in range(len(numeric_metrics), 3):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot categorical and numeric metrics separately\n",
    "categorical_metrics = [\"answer_correctness\"]\n",
    "plot_categorical_metrics(scores_df, categorical_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_metrics = [\"completeness\", \"faithfulness\", \"conciseness\"]\n",
    "plot_numeric_metrics(scores_df, numeric_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
